{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTLPNdKJ8sh7",
        "outputId": "c33b8dcf-cc99-40bd-ec13-bbb82d02a2da"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 27 05:39:34 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "534AMZF88v4W"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 50, 50)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def make_grad_hook():\n",
        "    '''\n",
        "    Function to keep track of gradients for visualization purposes, \n",
        "    which fills the grads list when using model.apply(grad_hook).\n",
        "    '''\n",
        "    grads = []\n",
        "    def grad_hook(m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            grads.append(m.weight.grad)\n",
        "    return grads, grad_hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBO1m7bF8wMj",
        "outputId": "c1e32f31-3075-41f8-88aa-ebda32706c54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESWtE-v-8wp-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n_epochs = 100\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "device = 'cuda'\n",
        "#device = 'cpu'\n",
        "n_classes = 12  ### Change\n",
        "image_size = 50\n",
        "image_shape = (1, 50, 50) ### Change\n",
        "\n",
        "## These files are created using: /media/forAll/Processed_Emotions/github/readingAllFiles.py\n",
        "dataFile = '/content/drive/MyDrive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether_ConditionalGAN_Normalized.npy'\n",
        "targetFile = '/content/drive/MyDrive/GANForEEG/SegmentWiseTargetDataWithSameDimension_AllTogether_ConditionalGAN.npy'\n",
        "maxValFile = '/content/drive/MyDrive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether_ConditionalGAN_maxVal.npy'\n",
        "\n",
        "data = np.load(dataFile)\n",
        "maxVal = np.load(maxValFile)\n",
        "data =  torch.from_numpy(data)\n",
        "target = torch.from_numpy(np.load(targetFile))     \n",
        "allIdxs = np.arange(len(data))\n",
        "import random\n",
        "random.shuffle(allIdxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEpjdQ2B82xJ"
      },
      "source": [
        "class Generator(nn.Module): ### Change\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(input_dim, hidden_dim * 6, kernel_size=3),\n",
        "            self.make_gen_block(hidden_dim * 6, hidden_dim * 4, kernel_size=3, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim, kernel_size=4),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        #print(f'shape of noise is = {noise.shape}')\n",
        "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, input_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "      n_samples: the number of samples to generate, a scalar\n",
        "      z_dim: the dimension of the noise vector, a scalar\n",
        "      device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, input_dim, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1t2Eq8d82uA"
      },
      "source": [
        "class Critic(nn.Module):  ### Change\n",
        "    '''\n",
        "    Critic Class\n",
        "    Values:\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_chan=1, hidden_dim=64):\n",
        "        super(Critic, self).__init__()\n",
        "        self.crit = nn.Sequential(\n",
        "            self.make_crit_block(im_chan, hidden_dim, kernel_size=3),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim, kernel_size=3),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim, kernel_size=3),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim * 2, kernel_size=3),\n",
        "            self.make_crit_block(hidden_dim * 2, 1, kernel_size=2, final_layer=True),\n",
        "        )\n",
        "    \n",
        "    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "\n",
        "        print(f'critic: input_channels = {input_channels}')\n",
        "        print(f'critic: output_channels = {output_channels}')\n",
        "\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the critic: Given an image tensor, \n",
        "        returns a 1-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with dimension (im_chan)'''\n",
        "        #print(f'image shape is = {image.shape}')\n",
        "        crit_pred = self.crit(image)\n",
        "        return crit_pred.view(len(crit_pred), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrTbpIF_82ql"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_one_hot_labels\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def get_one_hot_labels(labels, n_classes):\n",
        "    '''\n",
        "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
        "    Parameters:\n",
        "        labels: tensor of labels from the dataloader, size (?)\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    return nn.functional.one_hot(labels%n_classes)\n",
        "    #### END CODE HERE ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiAPySTz82mY",
        "outputId": "20b6a881-0139-49d5-b922-ec3ab5d74476"
      },
      "source": [
        "assert (\n",
        "    get_one_hot_labels(\n",
        "        labels=torch.Tensor([[0, 2, 1]]).long(),\n",
        "        n_classes=3\n",
        "    ).tolist() == \n",
        "    [[\n",
        "      [1, 0, 0], \n",
        "      [0, 0, 1], \n",
        "      [0, 1, 0]\n",
        "    ]]\n",
        ")\n",
        "# Check that the device of get_one_hot_labels matches the input device\n",
        "if torch.cuda.is_available():\n",
        "    assert str(get_one_hot_labels(torch.Tensor([[0]]).long().cuda(), 1).device).startswith(\"cuda\")\n",
        "    \n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ph4PBJhCE0E"
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: combine_vectors\n",
        "def combine_vectors(x, y):\n",
        "    '''\n",
        "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
        "    Parameters:\n",
        "      x: (n_samples, ?) the first vector. \n",
        "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
        "        but you shouldn't need to know the second dimension's size.\n",
        "      y: (n_samples, ?) the second vector.\n",
        "        Once again, in this assignment this will be the one-hot class vector \n",
        "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
        "    '''\n",
        "    # Note: Make sure this function outputs a float no matter what inputs it receives\n",
        "    #### START CODE HERE ####\n",
        "    combined = torch.cat((x, y), dim=1).to(torch.float32)   \n",
        "    #### END CODE HERE ####\n",
        "    return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeQz7NJdCEqg",
        "outputId": "39bc0aa5-4cde-4160-c1ea-2868c3c536b2"
      },
      "source": [
        "combined = combine_vectors(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))\n",
        "if torch.cuda.is_available():\n",
        "    # Check that it doesn't break with cuda\n",
        "    cuda_check = combine_vectors(torch.tensor([[1, 2], [3, 4]]).cuda(), torch.tensor([[5, 6], [7, 8]]).cuda())\n",
        "    assert str(cuda_check.device).startswith(\"cuda\")\n",
        "# Check exact order of elements\n",
        "assert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]]))\n",
        "# Tests that items are of float type\n",
        "print(f'type of class = {type(combined[0][0].item())}')\n",
        "assert (type(combined[0][0].item()) == float)\n",
        "# Check shapes\n",
        "combined = combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5));\n",
        "assert tuple(combined.shape) == (1, 12, 5)\n",
        "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()).shape) == (1, 30, 12)\n",
        "# Check that the float transformation doesn't happen after the inputs are concatenated\n",
        "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12)).shape) == (1, 30, 12)\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of class = <class 'float'>\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgYlDGwe82fu",
        "outputId": "0c38ccd0-7c9b-46be-f47e-adb88b893dc1"
      },
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "crit = Critic().to(device) \n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "crit = crit.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "critic: input_channels = 1\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 128\n",
            "critic: input_channels = 128\n",
            "critic: output_channels = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx2hnOrw82UD"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_gradient\n",
        "def get_gradient(crit, real, fake, epsilon):\n",
        "    '''\n",
        "    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
        "    Parameters:\n",
        "        crit: the critic model\n",
        "        real: a batch of real images\n",
        "        fake: a batch of fake images\n",
        "        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
        "    Returns:\n",
        "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "    '''\n",
        "    # Mix the images together\n",
        "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
        "\n",
        "    # Calculate the critic's scores on the mixed images\n",
        "    mixed_scores = crit(mixed_images)\n",
        "    \n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        # Note: You need to take the gradient of outputs with respect to inputs.\n",
        "        # This documentation may be useful, but it should not be necessary:\n",
        "        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "        #### START CODE HERE ####\n",
        "        inputs=mixed_images,\n",
        "        outputs=mixed_scores,\n",
        "        #### END CODE HERE ####\n",
        "        # These other parameters have to do with the pytorch autograd engine works\n",
        "        grad_outputs=torch.ones_like(mixed_scores), \n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    return gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCTaXNUf82Dp",
        "outputId": "5c746ae6-4251-47fd-e37f-6b8b6c5eed8b"
      },
      "source": [
        "# UNIT TEST\n",
        "# DO NOT MODIFY THIS\n",
        "def test_get_gradient(image_shape):\n",
        "    real = torch.randn(*image_shape, device=device) + 1\n",
        "    fake = torch.randn(*image_shape, device=device) - 1\n",
        "    epsilon_shape = [1 for _ in image_shape]\n",
        "    epsilon_shape[0] = image_shape[0]\n",
        "    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()\n",
        "    gradient = get_gradient(crit, real, fake, epsilon)\n",
        "    assert tuple(gradient.shape) == image_shape\n",
        "    assert gradient.max() > 0\n",
        "    assert gradient.min() < 0\n",
        "    return gradient\n",
        "\n",
        "#gradient = test_get_gradient((256, 1, width, width))\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d63rZd9T9dGL"
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: gradient_penalty\n",
        "def gradient_penalty(gradient):\n",
        "    '''\n",
        "    Return the gradient penalty, given a gradient.\n",
        "    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n",
        "    and penalize the mean quadratic distance of each magnitude to 1.\n",
        "    Parameters:\n",
        "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "    Returns:\n",
        "        penalty: the gradient penalty\n",
        "    '''\n",
        "    # Flatten the gradients so that each row captures one image\n",
        "    gradient = gradient.view(len(gradient), -1)\n",
        "\n",
        "    # Calculate the magnitude of every row\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    \n",
        "    # Penalize the mean squared distance of the gradient norms from 1\n",
        "    #### START CODE HERE ####\n",
        "    penalty = torch.mean((gradient_norm-1)**2)\n",
        "    #### END CODE HERE ####\n",
        "    return penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdJy9uMH9dDs",
        "outputId": "e00bec25-3584-439e-bb3f-4d20e600fcc1"
      },
      "source": [
        "# UNIT TEST\n",
        "def test_gradient_penalty(image_shape):\n",
        "    bad_gradient = torch.zeros(*image_shape)\n",
        "    bad_gradient_penalty = gradient_penalty(bad_gradient)\n",
        "    assert torch.isclose(bad_gradient_penalty, torch.tensor(1.))\n",
        "\n",
        "    image_size = torch.prod(torch.Tensor(image_shape[1:]))\n",
        "    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)\n",
        "    good_gradient_penalty = gradient_penalty(good_gradient)\n",
        "    assert torch.isclose(good_gradient_penalty, torch.tensor(0.))\n",
        "\n",
        "    random_gradient = test_get_gradient(image_shape)\n",
        "    random_gradient_penalty = gradient_penalty(random_gradient)\n",
        "    assert torch.abs(random_gradient_penalty - 1) < 0.1\n",
        "\n",
        "#test_gradient_penalty((256, 1, width, width))\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1QMaM_39dA9"
      },
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_gen_loss\n",
        "def get_gen_loss(crit_fake_pred):\n",
        "    '''\n",
        "    Return the loss of a generator given the critic's scores of the generator's fake images.\n",
        "    Parameters:\n",
        "        crit_fake_pred: the critic's scores of the fake images\n",
        "    Returns:\n",
        "        gen_loss: a scalar loss value for the current batch of the generator\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    gen_loss = -torch.mean(crit_fake_pred)\n",
        "    #### END CODE HERE ####\n",
        "    return gen_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJa07xfD9c-i",
        "outputId": "021a3be0-55de-4a6e-ea25-367cc1d14ba4"
      },
      "source": [
        "# UNIT TEST\n",
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.tensor(1.)), torch.tensor(-1.0)\n",
        ")\n",
        "\n",
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.rand(10000)), torch.tensor(-0.5), 0.05\n",
        ")\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ5UfRWt9c70"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_crit_loss\n",
        "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
        "    '''\n",
        "    Return the loss of a critic given the critic's scores for fake and real images,\n",
        "    the gradient penalty, and gradient penalty weight.\n",
        "    Parameters:\n",
        "        crit_fake_pred: the critic's scores of the fake images\n",
        "        crit_real_pred: the critic's scores of the real images\n",
        "        gp: the unweighted gradient penalty\n",
        "        c_lambda: the current weight of the gradient penalty \n",
        "    Returns:\n",
        "        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    crit_loss = -(torch.mean(crit_real_pred)-torch.mean(crit_fake_pred))+(c_lambda*gp)\n",
        "    #### END CODE HERE ####\n",
        "    return crit_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJZnlJXB9c5D",
        "outputId": "08535829-f7d1-4b67-ae58-e010fb1650de"
      },
      "source": [
        "# UNIT TEST\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(1.), torch.tensor(2.), torch.tensor(3.), 0.1),\n",
        "    torch.tensor(-0.7)\n",
        ")\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(20.), torch.tensor(-20.), torch.tensor(2.), 10),\n",
        "    torch.tensor(60.)\n",
        ")\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXM3b4srCm97"
      },
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_input_dimensions\n",
        "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
        "    '''\n",
        "    Function for getting the size of the conditional input dimensions \n",
        "    from z_dim, the image shape, and number of classes.\n",
        "    Parameters:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\n",
        "                (10 for MNIST)\n",
        "    Returns: \n",
        "        generator_input_dim: the input dimensionality of the conditional generator, \n",
        "                          which takes the noise and class vectors\n",
        "        discriminator_im_chan: the number of input channels to the discriminator\n",
        "                            (e.g. C x 28 x 28 for MNIST)\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    generator_input_dim = z_dim + n_classes\n",
        "    discriminator_im_chan = mnist_shape[0] + n_classes\n",
        "    #### END CODE HERE ####\n",
        "    return generator_input_dim, discriminator_im_chan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WpcoexVCoB5",
        "outputId": "9b32d194-a242-454a-eab8-d2cacc63e710"
      },
      "source": [
        "def test_input_dims():\n",
        "    gen_dim, disc_dim = get_input_dimensions(23, (12, 23, 52), 9)\n",
        "    assert gen_dim == 32\n",
        "    assert disc_dim == 21\n",
        "test_input_dims()\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iLJl8SjCrXe",
        "outputId": "24cce368-9162-4f51-b4b3-f9fb76fa8055"
      },
      "source": [
        "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, image_shape, n_classes)\n",
        "\n",
        "gen = Generator(input_dim=generator_input_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "crit = Critic(im_chan=discriminator_im_chan).to(device)\n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "crit = crit.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "critic: input_channels = 13\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 64\n",
            "critic: input_channels = 64\n",
            "critic: output_channels = 128\n",
            "critic: input_channels = 128\n",
            "critic: output_channels = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaxAjkvR9qMF",
        "outputId": "2fc65d76-44e8-47a1-be09-a546109def68"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED CELL\n",
        "import pdb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=int(data.shape[0]/batch_size))\n",
        "\n",
        "cur_step = 0\n",
        "generator_losses = []\n",
        "crit_losses = []\n",
        "n_classes = 12\n",
        "\n",
        "#UNIT TEST NOTE: Initializations needed for grading\n",
        "noise_and_labels = False\n",
        "fake = False\n",
        "\n",
        "fake_image_and_labels = False\n",
        "real_image_and_labels = False\n",
        "crit_fake_pred = False\n",
        "crit_real_pred = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f' Length of all the Indexes is = {len(allIdxs)}')\n",
        "n_epochs = 1\n",
        "for epoch in range(n_epochs):\n",
        "    # Dataloader returns the batches\n",
        "\n",
        "    for train_index, test_index in skf.split(data, target):\n",
        "\n",
        "        real = data[test_index]\n",
        "        labels = target[test_index]\n",
        "        cur_batch_size = len(real)\n",
        "        # Flatten the batch of real images from the dataset\n",
        "        real = real.to(device)        \n",
        "        \n",
        "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
        "        print(one_hot_labels.shape)\n",
        "        print(one_hot_labels[0,:])\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        print(image_one_hot_labels.shape)\n",
        "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, image_size,image_size)\n",
        "        print(image_one_hot_labels.shape)\n",
        "        print(image_one_hot_labels[0, :, :, :])\n",
        "        break\n",
        "        ### Update discriminator ###\n",
        "        # Zero out the discriminator gradients\n",
        "        crit_opt.zero_grad()\n",
        "        # Get noise corresponding to the current batch_size \n",
        "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        \n",
        "        # Now you can get the images from the generator\n",
        "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
        "        #        2) Generate the conditioned fake images\n",
        "       \n",
        "        #### START CODE HERE ####\n",
        "        noise_and_labels = torch.cat([fake_noise, one_hot_labels], dim=1)\n",
        "        #print(f'noise_and_labels = {noise_and_labels.shape}')\n",
        "        fake = gen(noise_and_labels)\n",
        "\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        # Make sure that enough images were generated\n",
        "        assert len(fake) == len(real)\n",
        "        # Check that correct tensors were combined\n",
        "        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n",
        "        # It comes from the correct generator\n",
        "        assert tuple(fake.shape) == (len(real), 1, image_shape[1], image_shape[2])\n",
        "\n",
        "        # Now you can get the predictions from the discriminator\n",
        "        # Steps: 1) Create the input for the discriminator\n",
        "        #           a) Combine the fake images with image_one_hot_labels, \n",
        "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
        "        #           b) Combine the real images with image_one_hot_labels\n",
        "        #        2) Get the discriminator's prediction on the fakes as crit_fake_pred\n",
        "        #        3) Get the discriminator's prediction on the reals as crit_real_pred\n",
        "        \n",
        "        #### START CODE HERE ####\n",
        "        #print(fake)\n",
        "        #print(real)\n",
        "        fake_image_and_labels = torch.cat([fake, image_one_hot_labels], dim=1)\n",
        "        real_image_and_labels = torch.cat([real, image_one_hot_labels], dim=1).to(torch.float32)\n",
        "        crit_fake_pred = crit(fake_image_and_labels)\n",
        "        crit_real_pred = crit(real_image_and_labels)\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        # Make sure shapes are correct \n",
        "        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], image_size ,image_size) ### Change\n",
        "        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], image_size ,image_size) ### Change\n",
        "        # Make sure that enough predictions were made\n",
        "        assert len(crit_real_pred) == len(real)\n",
        "        # Make sure that the inputs are different\n",
        "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
        "        # Shapes must match\n",
        "        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n",
        "        assert tuple(crit_fake_pred.shape) == tuple(crit_real_pred.shape)\n",
        "        \n",
        "        epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
        "        gradient = get_gradient(crit, real_image_and_labels, fake_image_and_labels.detach(), epsilon)\n",
        "        gp = gradient_penalty(gradient)\n",
        "        crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)                \n",
        "        crit_loss.backward(retain_graph=True)\n",
        "        crit_opt.step() \n",
        "\n",
        "        # Keep track of the average discriminator loss\n",
        "        crit_losses += [crit_loss.item()]\n",
        "\n",
        "        ### Update generator ###\n",
        "        # Zero out the generator gradients\n",
        "        gen_opt.zero_grad()\n",
        "\n",
        "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
        "        # This will error if you didn't concatenate your labels to your image correctly\n",
        "        crit_fake_pred = crit(fake_image_and_labels)\n",
        "        gen_loss = get_gen_loss(crit_fake_pred)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "        # Keep track of the generator losses\n",
        "        generator_losses += [gen_loss.item()]\n",
        "        #\n",
        "\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            crit_mean = sum(crit_losses[-display_step:]) / display_step\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {crit_mean}\")\n",
        "            print(f'Max Value for fake Image is = {torch.max(fake)}')\n",
        "            print(f'Max Value for real Image is = {torch.max(real)}')\n",
        "            fake = (fake + 1)*(maxVal/2)\n",
        "            print(f'Max Value for fake Image After Rescaling is = {torch.max(fake)}')\n",
        "            show_tensor_images(fake)\n",
        "            real = (real + 1)*(maxVal/2)\n",
        "            print(f'Max Value for real Image After Rescaling is = {torch.max(real)}')\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(crit_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Discriminator Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        elif cur_step == 0:\n",
        "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
        "        cur_step += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Length of all the Indexes is = 507474\n",
            "torch.Size([129, 12])\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "torch.Size([129, 12, 1, 1])\n",
            "torch.Size([129, 12, 50, 50])\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1,  ..., 1, 1, 1],\n",
            "         [1, 1, 1,  ..., 1, 1, 1],\n",
            "         [1, 1, 1,  ..., 1, 1, 1],\n",
            "         ...,\n",
            "         [1, 1, 1,  ..., 1, 1, 1],\n",
            "         [1, 1, 1,  ..., 1, 1, 1],\n",
            "         [1, 1, 1,  ..., 1, 1, 1]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}