{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Wasserstein GAN with Gradient Penalty (WGAN-GP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KD3ZgLs80vY"
      },
      "source": [
        "### Goals\n",
        "In this notebook, you're going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with the GANs that you have been using up until this point. Specifically, you'll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse.\n",
        "\n",
        "*Fun Fact: Wasserstein is named after a mathematician at Penn State, Leonid Vaseršteĭn. You'll see it abbreviated to W (e.g. WGAN, W-loss, W-distance).*\n",
        "\n",
        "### Learning Objectives\n",
        "1.   Get hands-on experience building a more stable GAN: Wasserstein GAN with Gradient Penalty (WGAN-GP).\n",
        "2.   Train the more advanced WGAN-GP model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8DDM6l9rZb"
      },
      "source": [
        "## Generator and Critic\n",
        "\n",
        "You will begin by importing some useful packages, defining visualization functions, building the generator, and building the critic. Since the changes for WGAN-GP are done to the loss function during training, you can simply reuse your previous GAN code for the generator and critic class. Remember that in WGAN-GP, you no longer use a discriminator that classifies fake and real as 0 and 1 but rather a critic that scores images with real numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sTpFE_eSk4s"
      },
      "source": [
        "#### Packages and Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4tkNheWMITc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d026c2a7-8c4d-4637-edf1-fe400d9bd483"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 50, 50)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def make_grad_hook():\n",
        "    '''\n",
        "    Function to keep track of gradients for visualization purposes, \n",
        "    which fills the grads list when using model.apply(grad_hook).\n",
        "    '''\n",
        "    grads = []\n",
        "    def grad_hook(m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            grads.append(m.weight.grad)\n",
        "    return grads, grad_hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A1M6kpnfxw"
      },
      "source": [
        "#### Generator and Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5tKdiiodIuQ",
        "outputId": "23ffdfcf-26bf-4893-84ec-e3d11cb66fdf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxX1kVeWYRHk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n_epochs = 100\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "#device = 'cuda'\n",
        "device = 'cpu'\n",
        "num_classes = 10\n",
        "image_size = 50\n",
        "\n",
        "'''transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "class CustomDataSet():\n",
        "    def __init__(self, dataFile, targetFile, transform):        \n",
        "        self.transform = transform        \n",
        "        self.data =  torch.from_numpy(np.load(dataFile))\n",
        "        self.target = torch.from_numpy(np.load(targetFile))        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "#img_folder_path = os.path.join(os.getcwd(), 'EEGFiles')\n",
        "dataFile = '/content/drive/My Drive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether_ConditionalGAN.npy'\n",
        "targetFile = '/content/drive/My Drive/GANForEEG/SegmentWiseTargetDataWithSameDimension_AllTogether_ConditionalGAN.npy'\n",
        "my_dataset = CustomDataSet(dataFile, targetFile, transform=transform)\n",
        "dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)'''\n",
        "\n",
        "dataFile = '/content/drive/My Drive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether_ConditionalGAN.npy'\n",
        "targetFile = '/content/drive/My Drive/GANForEEG/SegmentWiseTargetDataWithSameDimension_AllTogether_ConditionalGAN.npy'\n",
        "\n",
        "numpyD = np.load(dataFile)\n",
        "maxVal = np.max(numpyD)\n",
        "data = np.subtract(np.divide(numpyD.copy(), maxVal/2), 1)\n",
        "del numpyD\n",
        "data =  torch.from_numpy(data)\n",
        "target = torch.from_numpy(np.load(targetFile))     \n",
        "#data = torch.subtract(torch.divide(data, maxVal/2), 1)\n",
        "\n",
        "allIdxs = np.arange(len(data))\n",
        "import random\n",
        "random.shuffle(allIdxs)\n",
        "\n",
        "#data_ =  torch.from_numpy(np.load('SegmentWiseDeepDataWithSameDimension_AllTogether.npy'))\n",
        "#dataloader = DataLoader(data_ , batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeaSvfk6dErh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1bdb89c7-f8db-4be3-ae76-f8d52f2618b8"
      },
      "source": [
        "'''import numpy as np\n",
        "\n",
        "n_epochs = 100\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "device = 'cuda'\n",
        "num_classes = 10\n",
        "gen_embedding = 100\n",
        "\n",
        "data_ =  torch.from_numpy(np.load('/content/drive/My Drive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether.npy'))\n",
        "maxVal = torch.max(data_)\n",
        "data_ = torch.subtract(torch.divide(data_, maxVal/2), 1)\n",
        "dataloader = DataLoader(data_ , batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)\n",
        "################ Pay Attention To This ################\n",
        "\n",
        "nSamples = data_.shape[0]\n",
        "channels = data_.shape[2]\n",
        "image_size = data_.shape[2]'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"import numpy as np\\n\\nn_epochs = 100\\nz_dim = 64\\ndisplay_step = 500\\nbatch_size = 128\\nlr = 0.0002\\nbeta_1 = 0.5\\nbeta_2 = 0.999\\nc_lambda = 10\\ncrit_repeats = 5\\ndevice = 'cuda'\\nnum_classes = 10\\ngen_embedding = 100\\n\\ndata_ =  torch.from_numpy(np.load('/content/drive/My Drive/GANForEEG/SegmentWiseDeepDataWithSameDimension_AllTogether.npy'))\\nmaxVal = torch.max(data_)\\ndata_ = torch.subtract(torch.divide(data_, maxVal/2), 1)\\ndataloader = DataLoader(data_ , batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)\\n################ Pay Attention To This ################\\n\\nnSamples = data_.shape[0]\\nchannels = data_.shape[2]\\nimage_size = data_.shape[2]\""
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFwajQ3tGgI2"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=76):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )        \n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor,\n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        print(f'shape of noise is = {noise.shape}')\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        '''print(f' Shape of x matrix is = {x.shape}')\n",
        "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "        print(f' Shape of embedding matrix is = {embedding.shape}')\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        print(f' Shape of x matrix is = {x.shape}')'''\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "      n_samples: the number of samples to generate, a scalar\n",
        "      z_dim: the dimension of the noise vector, a scalar\n",
        "      device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9fScH98nkYH"
      },
      "source": [
        "#### Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA4AxGnmpuPq"
      },
      "source": [
        "class Critic(nn.Module):\n",
        "    '''\n",
        "    Critic Class\n",
        "    Values:\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_chan=13, hidden_dim=64):\n",
        "        super(Critic, self).__init__()\n",
        "        self.crit = nn.Sequential(\n",
        "            self.make_crit_block(im_chan, hidden_dim),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n",
        "        )\n",
        "        self.image_size = image_size\n",
        "        self.embed = nn.Embedding(num_classes, image_size*image_size)\n",
        "\n",
        "    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the critic: Given an image tensor, \n",
        "        returns a 1-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with dimension (im_chan)\n",
        "        '''\n",
        "        #embedding = self.embed(labels).view(labels.shape[0], 1, self.image_size, self.image_size)\n",
        "        #image = torch.cat([image, embedding], dim=1)\n",
        "        crit_pred = self.crit(image)\n",
        "        return crit_pred.view(len(crit_pred), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UJ6uYnl3N-D"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_one_hot_labels\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def get_one_hot_labels(labels, n_classes):\n",
        "    '''\n",
        "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
        "    Parameters:\n",
        "        labels: tensor of labels from the dataloader, size (?)\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    return nn.functional.one_hot(labels%n_classes)\n",
        "    #### END CODE HERE ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JFNYZef3PCe",
        "outputId": "4a6c6e51-fb0a-4395-8606-fb7b4ec10001"
      },
      "source": [
        "assert (\n",
        "    get_one_hot_labels(\n",
        "        labels=torch.Tensor([[0, 2, 1]]).long(),\n",
        "        n_classes=3\n",
        "    ).tolist() == \n",
        "    [[\n",
        "      [1, 0, 0], \n",
        "      [0, 0, 1], \n",
        "      [0, 1, 0]\n",
        "    ]]\n",
        ")\n",
        "# Check that the device of get_one_hot_labels matches the input device\n",
        "if torch.cuda.is_available():\n",
        "    assert str(get_one_hot_labels(torch.Tensor([[0]]).long().cuda(), 1).device).startswith(\"cuda\")\n",
        "    \n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk_8azSq3tF"
      },
      "source": [
        "## Training Initializations\n",
        "Now you can start putting it all together.\n",
        "As usual, you will start by setting the parameters:\n",
        "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   display_step: how often to display/visualize the images\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   lr: the learning rate\n",
        "  *   beta_1, beta_2: the momentum terms\n",
        "  *   c_lambda: weight of the gradient penalty\n",
        "  *   crit_repeats: number of times to update the critic per generator update - there are more details about this in the *Putting It All Together* section\n",
        "  *   device: the device type\n",
        "\n",
        "You will also load and transform the MNIST dataset to tensors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFLQ039u-qdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "09ffdfdc-1992-43d2-82bf-15f487a11fb0"
      },
      "source": [
        "########## Not To Be Executed\n",
        "\n",
        "'''n_epochs = 100\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "device = 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    MNIST('.', download=True, transform=transform),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"n_epochs = 100\\nz_dim = 64\\ndisplay_step = 500\\nbatch_size = 128\\nlr = 0.0002\\nbeta_1 = 0.5\\nbeta_2 = 0.999\\nc_lambda = 10\\ncrit_repeats = 5\\ndevice = 'cuda'\\n\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5,), (0.5,)),\\n])\\n\\ndataloader = DataLoader(\\n    MNIST('.', download=True, transform=transform),\\n    batch_size=batch_size,\\n    shuffle=True)\""
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Var22i_Ccs"
      },
      "source": [
        "Then, you can initialize your generator, critic, and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDFRZ8tg_Y57"
      },
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "crit = Critic().to(device) \n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "crit = crit.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFEi5BhVX5-P"
      },
      "source": [
        "## Gradient Penalty\n",
        "Calculating the gradient penalty can be broken into two functions: (1) compute the gradient with respect to the images and (2) compute the gradient penalty given the gradient.\n",
        "\n",
        "You can start by getting the gradient. The gradient is computed by first creating a mixed image. This is done by weighing the fake and real image using epsilon and then adding them together. Once you have the intermediate image, you can get the critic's output on the image. Finally, you compute the gradient of the critic score's on the mixed images (output) with respect to the pixels of the mixed images (input). You will need to fill in the code to get the gradient wherever you see *None*. There is a test function in the next block for you to test your solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4dkXnNtcv6"
      },
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_gradient\n",
        "def get_gradient(crit, real, fake, epsilon):\n",
        "    '''\n",
        "    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
        "    Parameters:\n",
        "        crit: the critic model\n",
        "        real: a batch of real images\n",
        "        fake: a batch of fake images\n",
        "        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
        "    Returns:\n",
        "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "    '''\n",
        "    # Mix the images together\n",
        "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
        "\n",
        "    # Calculate the critic's scores on the mixed images\n",
        "    mixed_scores = crit(mixed_images)\n",
        "    \n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        # Note: You need to take the gradient of outputs with respect to inputs.\n",
        "        # This documentation may be useful, but it should not be necessary:\n",
        "        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
        "        #### START CODE HERE ####\n",
        "        inputs=mixed_images,\n",
        "        outputs=mixed_scores,\n",
        "        #### END CODE HERE ####\n",
        "        # These other parameters have to do with the pytorch autograd engine works\n",
        "        grad_outputs=torch.ones_like(mixed_scores), \n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    return gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8av4TtbMtkTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9da916-8b19-48bc-bd55-1219ec2baa8a"
      },
      "source": [
        "# UNIT TEST\n",
        "# DO NOT MODIFY THIS\n",
        "def test_get_gradient(image_shape):\n",
        "    real = torch.randn(*image_shape, device=device) + 1\n",
        "    fake = torch.randn(*image_shape, device=device) - 1\n",
        "    epsilon_shape = [1 for _ in image_shape]\n",
        "    epsilon_shape[0] = image_shape[0]\n",
        "    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()\n",
        "    gradient = get_gradient(crit, real, fake, epsilon)\n",
        "    assert tuple(gradient.shape) == image_shape\n",
        "    assert gradient.max() > 0\n",
        "    assert gradient.min() < 0\n",
        "    return gradient\n",
        "\n",
        "#gradient = test_get_gradient((256, 1, width, width))\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PMRrMpRUK-"
      },
      "source": [
        "The second function you need to complete is to compute the gradient penalty given the gradient. First, you calculate the magnitude of each image's gradient. The magnitude of a gradient is also called the norm. Then, you calculate the penalty by squaring the distance between each magnitude and the ideal norm of 1 and taking the mean of all the squared distances.\n",
        "\n",
        "Again, you will need to fill in the code wherever you see *None*. There are hints below that you can view if you need help and there is a test function in the next block for you to test your solution.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<font size=\"3\" color=\"green\">\n",
        "<b>Optional hints for <code><font size=\"4\">gradient_penalty</font></code></b>\n",
        "</font>\n",
        "</summary>\n",
        "\n",
        "\n",
        "1.   Make sure you take the mean at the end.\n",
        "2.   Note that the magnitude of each gradient has already been calculated for you.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPwBH83IzCpS"
      },
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: gradient_penalty\n",
        "def gradient_penalty(gradient):\n",
        "    '''\n",
        "    Return the gradient penalty, given a gradient.\n",
        "    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n",
        "    and penalize the mean quadratic distance of each magnitude to 1.\n",
        "    Parameters:\n",
        "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
        "    Returns:\n",
        "        penalty: the gradient penalty\n",
        "    '''\n",
        "    # Flatten the gradients so that each row captures one image\n",
        "    gradient = gradient.view(len(gradient), -1)\n",
        "\n",
        "    # Calculate the magnitude of every row\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    \n",
        "    # Penalize the mean squared distance of the gradient norms from 1\n",
        "    #### START CODE HERE ####\n",
        "    penalty = torch.mean((gradient_norm-1)**2)\n",
        "    #### END CODE HERE ####\n",
        "    return penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahPfGMA2zABQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ac43aa-bdc1-41d2-df4f-b32db8a244ef"
      },
      "source": [
        "# UNIT TEST\n",
        "def test_gradient_penalty(image_shape):\n",
        "    bad_gradient = torch.zeros(*image_shape)\n",
        "    bad_gradient_penalty = gradient_penalty(bad_gradient)\n",
        "    assert torch.isclose(bad_gradient_penalty, torch.tensor(1.))\n",
        "\n",
        "    image_size = torch.prod(torch.Tensor(image_shape[1:]))\n",
        "    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)\n",
        "    good_gradient_penalty = gradient_penalty(good_gradient)\n",
        "    assert torch.isclose(good_gradient_penalty, torch.tensor(0.))\n",
        "\n",
        "    random_gradient = test_get_gradient(image_shape)\n",
        "    random_gradient_penalty = gradient_penalty(random_gradient)\n",
        "    assert torch.abs(random_gradient_penalty - 1) < 0.1\n",
        "\n",
        "#test_gradient_penalty((256, 1, width, width))\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sob-u9Z1X9sb"
      },
      "source": [
        "## Losses\n",
        "Next, you need to calculate the loss for the generator and the critic.\n",
        "\n",
        "For the generator, the loss is calculated by maximizing the critic's prediction on the generator's fake images. The argument has the scores for all fake images in the batch, but you will use the mean of them.\n",
        "\n",
        "There are optional hints below and a test function in the next block for you to test your solution.\n",
        "\n",
        "<details><summary><font size=\"3\" color=\"green\"><b>Optional hints for <code><font size=\"4\">get_gen_loss</font></code></b></font></summary>\n",
        "\n",
        "1. This can be written in one line.\n",
        "2. This is the negative of the mean of the critic's scores.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnJFs-qkMCA-"
      },
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_gen_loss\n",
        "def get_gen_loss(crit_fake_pred):\n",
        "    '''\n",
        "    Return the loss of a generator given the critic's scores of the generator's fake images.\n",
        "    Parameters:\n",
        "        crit_fake_pred: the critic's scores of the fake images\n",
        "    Returns:\n",
        "        gen_loss: a scalar loss value for the current batch of the generator\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    gen_loss = -torch.mean(crit_fake_pred)\n",
        "    #### END CODE HERE ####\n",
        "    return gen_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYVqG8bR6Hfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0a6530-e4bc-4567-80c9-611c79c8fe02"
      },
      "source": [
        "# UNIT TEST\n",
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.tensor(1.)), torch.tensor(-1.0)\n",
        ")\n",
        "\n",
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.rand(10000)), torch.tensor(-0.5), 0.05\n",
        ")\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt3MgH68TM1P"
      },
      "source": [
        "For the critic, the loss is calculated by maximizing the distance between the critic's predictions on the real images and the predictions on the fake images while also adding a gradient penalty. The gradient penalty is weighed according to lambda. The arguments are the scores for all the images in the batch, and you will use the mean of them.\n",
        "\n",
        "There are hints below if you get stuck and a test function in the next block for you to test your solution.\n",
        "\n",
        "<details><summary><font size=\"3\" color=\"green\"><b>Optional hints for <code><font size=\"4\">get_crit_loss</font></code></b></font></summary>\n",
        "\n",
        "1. The higher the mean fake score, the higher the critic's loss is.\n",
        "2. What does this suggest about the mean real score?\n",
        "3. The higher the gradient penalty, the higher the critic's loss is, proportional to lambda.\n",
        "\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jvbz1zDMTdu"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: get_crit_loss\n",
        "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
        "    '''\n",
        "    Return the loss of a critic given the critic's scores for fake and real images,\n",
        "    the gradient penalty, and gradient penalty weight.\n",
        "    Parameters:\n",
        "        crit_fake_pred: the critic's scores of the fake images\n",
        "        crit_real_pred: the critic's scores of the real images\n",
        "        gp: the unweighted gradient penalty\n",
        "        c_lambda: the current weight of the gradient penalty \n",
        "    Returns:\n",
        "        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    crit_loss = -(torch.mean(crit_real_pred)-torch.mean(crit_fake_pred))+(c_lambda*gp)\n",
        "    #### END CODE HERE ####\n",
        "    return crit_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxZey6fc5luf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7484935-f9e8-423e-9269-7dda584112e5"
      },
      "source": [
        "# UNIT TEST\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(1.), torch.tensor(2.), torch.tensor(3.), 0.1),\n",
        "    torch.tensor(-0.7)\n",
        ")\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(20.), torch.tensor(-20.), torch.tensor(2.), 10),\n",
        "    torch.tensor(60.)\n",
        ")\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x5wu7rUMlnZ"
      },
      "source": [
        "## Putting It All Together\n",
        "Before you put everything together, there are a few things to note.\n",
        "1.   Even on GPU, the **training will run more slowly** than previous labs because the gradient penalty requires you to compute the gradient of a gradient -- this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.\n",
        "2.   One important difference from earlier versions is that you will **update the critic multiple times** every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you're using). \n",
        "3.   WGAN-GP isn't necessarily meant to improve overall performance of a GAN, but just **increases stability** and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.\n",
        "\n",
        "\n",
        "<!-- Once again, be warned that this runs very slowly on a CPU. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab and make the runtime type GPU and replace\n",
        "`device = \"cpu\"`\n",
        "with\n",
        "`device = \"cuda\"`\n",
        "and make sure that your `get_noise` function uses the right device.  -->\n",
        "\n",
        "Here is a snapshot of what your WGAN-GP outputs should resemble:\n",
        "![MNIST Digits Progression](MNIST_WGAN_Progression.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "2hLiPXrmzwep",
        "outputId": "7b125cb6-afa0-4d56-ea3b-c76a4066c71b"
      },
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED CELL\n",
        "cur_step = 0\n",
        "generator_losses = []\n",
        "critic_losses = []\n",
        "n_classes = 12\n",
        "\n",
        "#UNIT TEST NOTE: Initializations needed for grading\n",
        "noise_and_labels = False\n",
        "fake = False\n",
        "\n",
        "fake_image_and_labels = False\n",
        "real_image_and_labels = False\n",
        "crit_fake_pred = False\n",
        "crit_real_pred = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Dataloader returns the batches\n",
        "    for i in np.arange(0, len(allIdxs), batch_size):\n",
        "        #print(i)\n",
        "        if (len(allIdxs)-i) < batch_size:\n",
        "            break\n",
        "\n",
        "        real = data[allIdxs[i:i+batch_size]]\n",
        "        labels = target[allIdxs[i:i+batch_size]]\n",
        "        cur_batch_size = len(real)\n",
        "        # Flatten the batch of real images from the dataset\n",
        "        real = real.to(device)        \n",
        "\n",
        "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, image_size,image_size)\n",
        "\n",
        "        ### Update discriminator ###\n",
        "        # Zero out the discriminator gradients\n",
        "        crit_opt.zero_grad()\n",
        "        # Get noise corresponding to the current batch_size \n",
        "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        \n",
        "        # Now you can get the images from the generator\n",
        "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
        "        #        2) Generate the conditioned fake images\n",
        "       \n",
        "        #### START CODE HERE ####\n",
        "        noise_and_labels = torch.cat([fake_noise, one_hot_labels], dim=1)\n",
        "        print(f'noise_and_labels = {noise_and_labels.shape}')\n",
        "        fake = gen(noise_and_labels)\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        # Make sure that enough images were generated\n",
        "        assert len(fake) == len(real)\n",
        "        # Check that correct tensors were combined\n",
        "        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n",
        "        # It comes from the correct generator\n",
        "        assert tuple(fake.shape) == (len(real), 1, 28, 28)\n",
        "\n",
        "        # Now you can get the predictions from the discriminator\n",
        "        # Steps: 1) Create the input for the discriminator\n",
        "        #           a) Combine the fake images with image_one_hot_labels, \n",
        "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
        "        #           b) Combine the real images with image_one_hot_labels\n",
        "        #        2) Get the discriminator's prediction on the fakes as crit_fake_pred\n",
        "        #        3) Get the discriminator's prediction on the reals as crit_real_pred\n",
        "        \n",
        "        #### START CODE HERE ####\n",
        "        fake_image_and_labels = torch.cat([fake, image_one_hot_labels], dim=1)\n",
        "        real_image_and_labels = torch.cat([real, image_one_hot_labels], dim=1)\n",
        "        crit_fake_pred = crit(fake_image_and_labels)\n",
        "        crit_real_pred = crit(real_image_and_labels)\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        # Make sure shapes are correct \n",
        "        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
        "        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
        "        # Make sure that enough predictions were made\n",
        "        assert len(crit_real_pred) == len(real)\n",
        "        # Make sure that the inputs are different\n",
        "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
        "        # Shapes must match\n",
        "        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n",
        "        assert tuple(crit_fake_pred.shape) == tuple(crit_real_pred.shape)\n",
        "        \n",
        "        epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
        "        gradient = get_gradient(crit, real, fake.detach(), epsilon)\n",
        "        gp = gradient_penalty(gradient)\n",
        "        crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
        "                \n",
        "        crit_loss.backward(retain_graph=True)\n",
        "        crit_opt.step() \n",
        "\n",
        "        # Keep track of the average discriminator loss\n",
        "        crit_losses += [crit_loss.item()]\n",
        "\n",
        "        ### Update generator ###\n",
        "        # Zero out the generator gradients\n",
        "        gen_opt.zero_grad()\n",
        "\n",
        "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
        "        # This will error if you didn't concatenate your labels to your image correctly\n",
        "        crit_fake_pred = crit(fake_image_and_labels)\n",
        "        gen_loss = criterion(crit_fake_pred, torch.ones_like(crit_fake_pred))\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "        # Keep track of the generator losses\n",
        "        generator_losses += [gen_loss.item()]\n",
        "        #\n",
        "\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            crit_mean = sum(crit_losses[-display_step:]) / display_step\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {crit_mean}\")\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(crit_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Discriminator Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        elif cur_step == 0:\n",
        "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
        "        cur_step += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noise_and_labels = torch.Size([128, 76])\n",
            "shape of noise is = torch.Size([128, 76])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7c7f2fb546b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mnoise_and_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfake_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'noise_and_labels = {noise_and_labels.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_and_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m#### END CODE HERE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d2e2fc52d3e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     51\u001b[0m         '''\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'shape of noise is = {noise.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         '''print(f' Shape of x matrix is = {x.shape}')\n\u001b[1;32m     55\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 64, 1, 1]' is invalid for input of size 9728"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXptQZcwrBrq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cur_step = 0\n",
        "generator_losses = []\n",
        "critic_losses = []\n",
        "n_epochs = 1\n",
        "for epoch in range(n_epochs):\n",
        "    # Dataloader returns the batches\n",
        "    #for real, labels in tqdm(dataloader):\n",
        "\n",
        "    for i in np.arange(0, len(allIdxs), batch_size):\n",
        "        #print(i)\n",
        "        if (len(allIdxs)-i) < batch_size:\n",
        "            break\n",
        "\n",
        "        real = data[allIdxs[i:i+batch_size]]\n",
        "        labels = target[allIdxs[i:i+batch_size]]\n",
        "    #for real, labels in zip(newData, newTarget):\n",
        "        print(f' Size of real is = {real.shape}')\n",
        "        print(f' Lables is = {labels}')\n",
        "        cur_batch_size = len(real)\n",
        "        print(f'device = {device}')\n",
        "        real = real.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        mean_iteration_critic_loss = 0\n",
        "        for _ in range(crit_repeats):\n",
        "            ### Update critic ###\n",
        "            crit_opt.zero_grad()\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "            print(f'fake noise = {fake_noise.shape}')\n",
        "            fake = gen(fake_noise, labels)\n",
        "            crit_fake_pred = crit(fake.detach(), labels)\n",
        "            crit_real_pred = crit(real, labels)\n",
        "\n",
        "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
        "            gradient = get_gradient(crit, real, fake.detach(), epsilon, labels)\n",
        "            gp = gradient_penalty(gradient)\n",
        "            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
        "\n",
        "            # Keep track of the average critic loss in this batch\n",
        "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
        "            # Update gradients\n",
        "            crit_loss.backward(retain_graph=True)\n",
        "            # Update optimizer\n",
        "            crit_opt.step()\n",
        "        critic_losses += [mean_iteration_critic_loss]\n",
        "\n",
        "        ### Update generator ###\n",
        "        gen_opt.zero_grad()\n",
        "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        fake_2 = gen(fake_noise_2, labels)\n",
        "        crit_fake_pred = crit(fake_2, labels)\n",
        "        \n",
        "        gen_loss = get_gen_loss(crit_fake_pred)\n",
        "        gen_loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        gen_opt.step()\n",
        "\n",
        "        # Keep track of the average generator loss\n",
        "        generator_losses += [gen_loss.item()]\n",
        "\n",
        "        ### Visualization code ###\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Critic Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY4WaLazXGp-"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}